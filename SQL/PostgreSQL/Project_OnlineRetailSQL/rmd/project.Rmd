---
title: "Online Retail"
author: "Data Engineering Trooper"
date: '`r format(Sys.time(), "%Y-%m-%d %H:%M")`'
header-includes:
- \usepackage{float} #use the 'float' package
- \floatplacement{figure}{H} #make every figure with caption = h
output:
    html_document:
        number_sections: true
        self_contained: true
        code_folding: show
        toc: true
        toc_float:
            collapsed: true
            smooth_scroll: false
    pdf_document:
        number_sections: true
        toc: true
        fig_cap: yes
        keep_tex: yes
urlcolor: blue
---

<style>
body{
font-size:14pt
}

.important{
  background-color : lightblue; 
  font-size:16pt
}

.backgrnd{
background-color : red
}
</style>

# Initial Setup

This code chunk loads the necessary packages and sets up a connection with the database running on localhost.

```{r setup, message = FALSE}
# Loading packages
library(RPostgres)
library(DBI)

# Setting up code chunks
knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = TRUE)

# Connecting to the database 'onlineretail' on localhost
con <- DBI::dbConnect(
  Postgres(),
  dbname = 'onlineretail',
  host = 'localhost',
  port = 5432,
  user = 'csde',
  password = 'gis'
)
```

# Explatory Analysis

## Data Description

Before we start with the analysis I'd like to thank you [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail#) for providing the dataset.

<p class = "important">This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.</p>


Let's start with column names first and then describe what each column stands for. The following code chunk will print out column names for us.

```{sql s1, connection = con}
SELECT json_object_keys(
         to_json(
           json_populate_record(NULL::main_table, '{}'::JSON
          )
        )
      ) AS column_names;
```

As we can see that the table has 8 columns as listed above. Now let's jot down what each column stands for.

`InvoiceNo`: Invoice number. A 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.

`StockCode`: Product (item) code. A 5-digit integral number uniquely assigned to each distinct product.

`Description`: Product (item) name.

`Quantity`: The quantities of each product (item) per transaction.

`InvoiceDate`: Invoice Date and time. The day and time when each transaction was generated.

`UnitPrice`: Unit price. Product price per unit in sterling.

`CustomerID`: Customer number. A 5-digit integral number uniquely assigned to each customer.

`Country`: Country name. The name of the country where each customer resides.

## Data Exploration

Let's have a look at what kind of data we are working with. We'll print first
10 records.


```{sql s2, connection = con}
SELECT * 
FROM main_table
LIMIT 10;
```

From the output displayed above, it seems like everything is okay but
let's deep dive into exploratory analysis.

### Null Values

[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail#) didn't provide any details regarding the missing values so We'll go ahead and check for ourselves. 

We can use the following query to find out the missing values in the columns. 

```{sql stmp_1, connection = con, eval=FALSE}
SELECT COUNT(*) AS null_rows
FROM table_name
WHERE column_name IS NULL;
```

*Similar query can be used to find out missing values in other columns. Let's see the below table which contains all the information regarding missing values in the columns.*

```{sql s3, connection = con, echo = F, output.var="ino_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE invoiceno IS NULL;
```

```{sql s4, connection = con, echo = F, output.var="stkcode_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE stockcode IS NULL;
```

```{sql s5, connection = con, output.var = "desc_nulls", echo=F}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE description IS NULL;
```

```{sql stmp_2, connection = con, output.var = "total_rows", echo = F}
SELECT COUNT(*)
FROM main_table;
```

```{sql s6, connection = con, , echo = F, output.var="qnty_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE quantity IS NULL;
```

```{sql s7, connection = con, echo = F, output.var="idate_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE invoicedate IS NULL;
```

```{sql s8, connection = con, echo = F, output.var="uprice_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE unitprice IS NULL;
```

```{sql s9, connection = con, output.var = "cid_nulls", echo=F}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE customerid IS NULL;
```

```{sql s10, connection = con, echo=FALSE, output.var="cntry_nulls"}
SELECT COUNT(*) AS null_rows
FROM main_table
WHERE country IS NULL;
```


```{r null_values, echo=FALSE, table.caption = "Null values"}
# Missing values vector
nulls <- c(ino_nulls, stkcode_nulls, desc_nulls, qnty_nulls, idate_nulls,
           uprice_nulls, cid_nulls, cntry_nulls)

# Value extraction for nulls
nulls_values <- c()

# Calculating the percentage of missing values
percentOfMissingValues <- c()

for(i in 1:length(nulls)) {   
  tmp = (nulls[i]$null_rows * 100)/total_rows$count
  nulls_values[i] <- format(nulls[i]$null_rows)
  percentOfMissingValues[i] <- paste(format(round(tmp, 1)), "%")
}

null_df <- data.frame(
  Columns = c("invoiceno", "stockcode", "description", "quantity",
                  "invoicedate", "unitprice", "customerid", "country"),
  "Null Values" = nulls_values,
  "Percentage Of Null Values" = percentOfMissingValues,
  stringsAsFactors = FALSE,
  check.names = FALSE
)

knitr::kable(null_df, caption = "Table 1 : Null values")
```


<br/><p class = "important">As we can see in the above table that except for `description` and `customerid`, the rest of the columns don't have missing values in them.</p> *The percentage of the missing values will be an important factor later on when it comes to deciding what to do with these missing values for example remove the records altogether or impute these records with some value which would make sense in such data space.* 


### Dealing with Null Values

  * The percentage of missing values in `description` is really low(*See Table 1 : Null values*) and `description` column won't hold much power in the analysis we will be conducting. We will be imputing the missing values with something really basic like "No description provided".

  * The astounding number of rows with missing `customerid`((*See Table 1 : Null values*)) values is a big data quality problem but it also could be explained by proposing that these rows could represent **guest customers which didn't make an account or register on the platform** but such customers did used the platform to buy stuff. We can not simply remove all the records with missing values. We can impute them with something like `'11111'`  or `'GUEST'`(*of course after checking whether that value already exists or not*) which would represent guest customers on the platform.
  
  * Two problems I encountered while I was exploring the data was that 
    1. Some of the **records have** `unitprice` **with the value** `0`. Now, the `unitprice`(Product price per unit in sterling) shouldn't be `0` for any products. 
    2. Several of the `quantity` values are **negative** which is not practical.

Let's deal with above-mentioned problems using SQL. Before we make any changes, it is always a good idea not to change the original data source be it a table, a dataframe, or a file. So, let's first make a copy of the table and then do our data cleaning tasks on the copy.

```{sql s11, connection = con}
CREATE TABLE IF NOT EXISTS cleaned_data
AS SELECT *
FROM main_table
```

Let's handle the data cleaning task column by column starting with `description`.

<ol>
<li>`description`</li> 
<br/>
<ul>
<li>Let's first impute the missing values in the columns.</li>

```{sql s12, connection = con}
UPDATE cleaned_data
SET description = 'No Description Provided'
WHERE description IS NULL
```

<li> Let's check if there are any missing values lefts in the `description` column. </li>

```{sql s13, connection = con}
SELECT COUNT(*) AS null_rows
FROM cleaned_data
WHERE description IS NULL
```

<li>Also, we'll check by comparing the count of 'No Description Provided' with Null count we got in *Table 1 : Null Values*.</li>
  
```{sql s14, connection = con}
SELECT COUNT(*) AS count_new_value
FROM cleaned_data
WHERE description = 'No Description Provided'
```

<p class = "important">From the above results, we can conclude that there are no missing values present in the column and all the missing values have been imputed successfully. </p>

<br/>

</ul>
<li>`customerid`</li>
<br/>
As we have seen above that almost 25% of the rows have missing values in `customerid`. The best thing to do here would be to impute them with some default value. One of the reasons behind this is inclusion these rows(which is a huge piece of data) in data analysis. Also, the imputing of the default value could help discover patterns in the data. So, let's go ahead and fix `customerid` column.
<ul>
<li>First check the existing values present in `customerid`.</li>

```{sql s15, connection = con}
SELECT customerid, COUNT(*) counts
FROM cleaned_data
GROUP BY customerid
ORDER BY counts DESC
```

Again, we can see that `135080` rows have missing values which we have already said could represent guest customers visiting the platform. Let's change the `customerid` of said rows to `GUEST`.

<li>Changing the value of `customerid` to `GUEST`.</li>

```{sql s16, connection = con}
UPDATE cleaned_data
SET customerid = 'guest' 
WHERE customerid IS NULL
```

<li>Let's check the distribution of the values in `customerid`.</li>

```{sql s17, connection = con}
SELECT customerid, COUNT(*) counts
FROM cleaned_data
GROUP BY customerid
ORDER BY counts DESC
```

</ul>

<br/>

<li>`unitprice`</li>
<br/>
<ul>
<li>Let's explore the values of `unitprice`</li>

```{sql s18, connection = con}
SELECT COUNT(*) AS neg_zero_counts
FROM cleaned_data
WHERE unitprice <= 0
```




</ul>
</ol>

# Disconnecting Database

Here, we are ending the connection made to the database since we are done with the analysis. 

```{r dbdisconnect}
DBI::dbDisconnect(con)
```